---
title: "project"
output: html_document
date: "2023-06-16"
---

```{r,include=FALSE}
pkg <- c('dplyr', 'scales', 'tidyr', 'ggplot2', 'data.table', 'readr', 
         'tidymodels', 'ranger', 'knitr', 'summarytools', 
         'tidyverse', 'recipes', 'stats', 'janitor','FactoMineR','factoextra','cluster','car','pheatmap','randomForest','Boruta')

check_and_install <- function(p) {
  if(!require(p, character.only = TRUE)) {
    install.packages(p, dependencies = TRUE)
    library(p, character.only = TRUE)
  }
}

sapply(pkg, check_and_install)

```

## First Section - Linear Regression Model

```{r,include=FALSE}
# Load the datasets
survey_data_regression <- read_csv("data/mxmh_survey_results.csv")
spotify_data_regression <- read_csv("data/spotify.csv")
```

**Clean features name - using janitor**

```{r,incluse=FALSE}
survey_data_regression <- survey_data_regression %>%
  clean_names()
survey_data_regression <- rename(survey_data_regression, frequency_r_n_b = frequency_r_b)
survey_data_regression <- select(survey_data_regression, -c(frequency_video_game_music, frequency_rap, frequency_lofi))
```

**Calculate mean attribute scores to each genre**

```{r,warning=FALSE}
# Define the genres of interest
genres_of_interest <- c("classical", "country", "edm", "folk", "gospel", "hip_hop", 
                        "jazz", "k_pop", "latin", "metal", "pop", "r_n_b", "rock")
features <- c("danceability", "energy", "key", "loudness", "speechiness",
              "acousticness", "instrumentalness", "liveness", "valence", "tempo")

# Filter spotify_data_regression for the genres of interest
spotify_data_regression <- spotify_data_regression %>%
  filter(track_genre %in% genres_of_interest)

# Calculate mean for each genre in filtered_spotify_data_regression
spotify_means <- spotify_data_regression %>%
  group_by(track_genre) %>%
  summarise(across(features, mean, na.rm = TRUE), .groups = 'drop')

# Define the genre columns in survey_data_regression
genre_cols <- c("weight_classical", "weight_country", "weight_edm", "weight_folk", "weight_gospel",
                "weight_hip_hop", "weight_jazz", "weight_k_pop", "weight_latin", "weight_metal",
                "weight_pop", "weight_r_n_b", "weight_rock")

spotify_means <- spotify_means %>%
  mutate(track_genre = paste0("frequency_", track_genre))
specific_genres <- c("classical", "country", "edm", "folk", "gospel", "hip_hop", "jazz", "k_pop", "latin", "metal", "pop", "r_n_b", "rock")
# Create a list of genre columns in survey_data_regression
genre_cols <- paste0("frequency_", specific_genres)

```

**Define weight to each listening frequency value**

```{r}
freq_map <- c("Never" = 0, "Rarely" = 0.33, "Sometimes" = 0.67, "Very frequently" = 1)

survey_data_regression <- survey_data_regression %>%
  mutate(across(starts_with("frequency_"), ~ freq_map[.])) 
```

**Calculate the weighted average of music attributes for each person based on the listening frequency of each genre (individual attribute score)**

```{r}


features <- c("danceability", "energy", "loudness", "speechiness", "acousticness", "instrumentalness", "liveness", "valence","tempo")

for (i in 1:nrow(survey_data_regression)) {
  # Initialize variables
  multi <- rep(0, length(features))
  division <- 0
  for (feature in features) {
    multi <- rep(0, length(features))
    division <- 0
    for (genre_col in genre_cols) {
      weight <- as.numeric(survey_data_regression[i, genre_col]) * as.numeric(survey_data_regression[i, "hours_per_day"])
      weights <- rep(weight, length(features))
      
      # Get genre means for the current genre and feature
      genre_means <- as.numeric(unlist(spotify_means[spotify_means$track_genre == genre_col, feature]))

      # Skip iteration if genre_means is empty
      if (length(genre_means) == 0)
        next
      
      # Multiply weight with each feature value and accumulate the sum
      multi <- multi + (weights * genre_means)
      
      # Accumulate the weight in the division variable
      division <- division + weight
    }
      # Create a vector of the same length as features, filled with the division value
  divisions <- rep(division, length(features))
  
  # Calculate the weighted average
  weighted_average <- ifelse(division == 0, 0, multi / divisions)
  
  # Add the weighted average to the survey_data_regression
  survey_data_regression[i, paste0(feature,"_score")] <- weighted_average
  }
  
}

```

## Feature engeneering - Clustering Approach

**Dimensionality Reduction**

```{r,include=FALSE}
numeric_columns <- sapply(survey_data_regression, is.numeric)
library(tidymodels)

survey_data_regression <- survey_data_regression  %>%
  mutate(
    bpm = replace_na(bpm, median(bpm, na.rm = TRUE)),
    age = replace_na(age, median(age, na.rm = TRUE))
  )

# Calculate the lower and upper bounds for Winsorization
lower_bound <- quantile(survey_data_regression$bpm, 0.05)
upper_bound <- quantile(survey_data_regression$bpm, 0.95)

# Winsorize the bpm feature
survey_data_regression$bpm <- ifelse(survey_data_regression$bpm < lower_bound, lower_bound, survey_data_regression$bpm)
survey_data_regression$bpm <- ifelse(survey_data_regression$bpm > upper_bound, upper_bound, survey_data_regression$bpm)

# Calculate and print the column names and ranges
column_names <- names(survey_data_regression)
for (column in column_names) {
  if (is.numeric(survey_data_regression[[column]])) {
    cat("Column:", column, "\n")
    cat("Range:", min(survey_data_regression[[column]]), max(survey_data_regression[[column]]), "\n\n")
  }
}


```

**Feture set 1 - attributes scores**

```{r}

# Feature Set 1
attributes_scores = paste0(features,"_score")

# Perform PCA for dimensionality reduction
res.pca1 <- PCA(survey_data_regression[,attributes_scores], scale.unit = TRUE, ncp = 5, graph = FALSE)

# Plot the variance explained by each principal component
fviz_eig(res.pca1, addlabels = TRUE, ylim = c(0, 100)) 
```

```{r}
# Contributions of variables to PC1
fviz_contrib(res.pca1, choice = "var", axes = 1, top = 10)

# Contributions of variables to PC2
fviz_contrib(res.pca1, choice = "var", axes = 2, top = 10)

# Contributions of variables to PC3
fviz_contrib(res.pca1, choice = "var", axes = 3, top = 10)
```

**Check the correlation of each PCA to anxiety,depression,ocd,insomnia**

```{r}
# Create a new data frame
pca_df <- data.frame(PC1 = res.pca1$ind$coord[,1],
                     PC2 = res.pca1$ind$coord[,2],
                     PC3 = res.pca1$ind$coord[,3],
                     anxiety = survey_data_regression$anxiety,
                     depression = survey_data_regression$depression,
                     insomnia = survey_data_regression$insomnia,
                     ocd = survey_data_regression$ocd)

# Compute the correlation
correlation_matrix <- cor(pca_df)

# Visualize the correlation matrix using corrplot
correlation_matrix
```

**Now, we will try diffrent approach. K-Mean Clustering**

```{r}
attributs <- survey_data_regression[, attributes_scores]
attributs$depression <- survey_data_regression$depression
attributs$anxiety <- survey_data_regression$anxiety
attributs$insomnia <- survey_data_regression$insomnia
attributs$ocd <- survey_data_regression$ocd

cor(attributs)
```

```{r}
# Set the seed value
set.seed(92)

feature_set1 <- scale(survey_data_regression[, attributes_scores])

# Perform K-means clustering by the attributes scores values
k2 <- kmeans(feature_set1, centers = 4)

# Visualize the clusters
clust_plot <- fviz_cluster(k2, data = feature_set1,show.clust.cent = TRUE)
print(clust_plot)
```

**We will remove the 7 observations of cluster 2**

```{r}
# Count the frequencies of cluster assignments
filterd_data <- survey_data_regression[k2$cluster != 2, ] 
```

**Now we will preform K-means with 3 K's**

```{r}

# Set the seed value
set.seed(123)
# Min-Max normalization
feature_set1 <- sapply(filterd_data[, attributes_scores], function(x) (x - min(x)) / (max(x) - min(x)))


# Perform K-means clustering by the attributes scores values
k3 <- kmeans(feature_set1, centers = 3)

# Visualize the clusters
clust_plot <- fviz_cluster(k3, data = feature_set1,show.clust.cent = TRUE)
print(clust_plot)
```

filterd_data

```{r}
# Get unique cluster assignments
unique_clusters <- unique(k3$cluster)

# Create an empty matrix to store the correlation coefficients
correlation_matrix <- matrix(NA, nrow = length(unique_clusters), ncol = length(attributes_scores))

# Assign 'depression_clusterX' as the row names
rownames(correlation_matrix) <- paste('depression_cluster', seq_len(length(unique_clusters)), sep = '')

# Assign attributes_scores as the column names
colnames(correlation_matrix) <- attributes_scores

# Calculate the correlation coefficient for each cluster's features with depression
for (i in 1:length(unique_clusters)) {
  cluster_indices <- which(k3$cluster == unique_clusters[i])
  for (j in 1:length(attributes_scores)) {
    correlation_matrix[i, j] <- cor(survey_data_regression[cluster_indices, attributes_scores[j]], survey_data_regression[cluster_indices, "depression"])
  }
}
filterd_data$attribute_cluster <- as.factor(k3$cluster)

# Print the correlation matrix
print(correlation_matrix)
```

```{r}
# Calculate means of each attribute for each cluster
cluster_means <- aggregate(feature_set1, 
                           by=list(cluster=filterd_data$attribute_cluster), 
                           FUN=mean)

# View the means
print(cluster_means)

# Generate the heatmap
pheatmap(cluster_means[-1],  # Exclude the cluster column from the heatmap
         scale = "row",  # Scale values by row
         clustering_distance_rows = "euclidean",
         clustering_distance_cols = "euclidean",
         clustering_method = "complete",
         main = "Heatmap of attribute means by attributes scores clusters",
         color = colorRampPalette(c("navy", "white", "firebrick3"))(50))

# group 1 - high dancabillty , high energy, hight loudness, moderate speechiness,low acousticness, very low instrumentlness 
# group 2 -  moderate energy, moderate loudness, low energy,high acousticness,low instumentalness
# group 3 - moderate dancabillty, moderate energy, low loudness,
```

```{r}

# Melt the data into a format suitable for ggplot
melted_df <- reshape2::melt(filterd_data, id.vars = "attribute_cluster", measure.vars = c("depression", "ocd", "insomnia", "anxiety"))

# Create a boxplot for each pair of (cluster, mental problem)
ggplot(melted_df, aes(x=attribute_cluster, y=value)) + 
  geom_boxplot(aes(fill=attribute_cluster), outlier.shape = NA) +  # Hide outliers for better visualization
  facet_wrap(~variable, scales = "free", nrow = 4) +  # Use facets to arrange the plots
  theme_bw() + 
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"),
        strip.text = element_text(size = 14),
        legend.position = "none") +  # Hide legend
  labs(x = "Cluster", y = "Score", title = "Comparison of mental health scores across attributes clusters") +
  coord_cartesian(ylim = c(0, 10))  # Limit y axis to the range of mental health scores
```

**Anova tests between the attributes clusters & mental health scores**

```{r}
# Run an ANOVA to test if there is a significant difference in depression scores between clusters
anova_test <- aov(depression ~ as.factor(attribute_cluster), data = filterd_data)
summary(anova_test)

# Bin depression scores into categories
filterd_data$depression_cat <- cut(filterd_data$depression, breaks = 3, labels = c("Low", "Medium", "High"))

# Form a contingency table
cont_table <- table(filterd_data$attribute_cluster, filterd_data$depression_cat)

# Perform Chi-square test
chi_test <- chisq.test(cont_table)

# Get p-value
chi_test$p.value

```

**Feture set 2 - Genere frequency**

```{r}

# Perform PCA for dimensionality reduction
res.pca1 <- PCA(survey_data_regression[,genre_cols], scale.unit = TRUE, ncp = 5, graph = FALSE)

# Plot the variance explained by each principal component
fviz_eig(res.pca1, addlabels = TRUE, ylim = c(0, 100)) 
```

```{r}
# Contributions of variables to PC1
fviz_contrib(res.pca1, choice = "var", axes = 1, top = 10)

# Contributions of variables to PC2
fviz_contrib(res.pca1, choice = "var", axes = 2, top = 10)

# Contributions of variables to PC3
fviz_contrib(res.pca1, choice = "var", axes = 3, top = 10)
```

**We will need at to reduce to at least 4 dimensions (for maintain 60% of the variation)**

```{r}
# Create a new data frame
pca_df <- data.frame(PC1 = res.pca1$ind$coord[,1],
                     PC2 = res.pca1$ind$coord[,2],
                     PC3 = res.pca1$ind$coord[,3],
                     PC4 = res.pca1$ind$coord[,4],
                     anxiety = survey_data_regression$anxiety,
                     depression = survey_data_regression$depression,
                     insomnia = survey_data_regression$insomnia,
                     ocd = survey_data_regression$ocd)

# Compute the correlation
correlation_matrix <- cor(pca_df)

# Visualize the correlation matrix using corrplot
correlation_matrix
```

```{r}
# Set the seed value
set.seed(92)

feature_set2 <- sapply(survey_data_regression[,genre_cols], function(x) (x - min(x)) / (max(x) - min(x)))

library(cluster)

# Define the range of k values to try
k_values <- 2:10

# Initialize an empty vector to store silhouette coefficients
silhouette_values <- numeric(length(k_values))

# Compute silhouette coefficient for each k
for (i in seq_along(k_values)) {
  k <- k_values[i]
  km_result <- kmeans(feature_set2, centers = k)
  silhouette_result <- silhouette(km_result$cluster, dist(feature_set2))
  silhouette_values[i] <- mean(silhouette_result[, 'sil_width'])
}

# Find the optimal k
optimal_k <- k_values[which.max(silhouette_values)]

# Print the optimal k
print(paste0("The optimal number of clusters is ", optimal_k))

# Plot silhouette coefficients for different k values
plot(k_values, silhouette_values, type = 'b', 
     xlab = 'Number of clusters (k)', ylab = 'Average silhouette coefficient',
     main = 'Optimal number of clusters with silhouette method')

rownames(feature_set2) <- NULL

# Perform K-means clustering by the attributes scores values
k2 <- kmeans(feature_set2, centers = 2)

# Visualize the clusters
clust_plot <- fviz_cluster(k2, data = feature_set2,show.clust.cent = TRUE)
print(clust_plot)
```

```{r}
filterd_data2 <- survey_data_regression[,genre_cols]
```

```{r}
# Get unique cluster assignments
unique_clusters <- unique(k2$cluster)

# Create an empty matrix to store the correlation coefficients
correlation_matrix <- matrix(NA, nrow = length(unique_clusters), ncol = ncol(survey_data_regression[, genre_cols]))

# Calculate the correlation coefficient for each cluster's features with depression
for (i in 1:length(unique_clusters)) {
  cluster_indices <- which(k2$cluster == unique_clusters[i])
  correlation_matrix[i, ] <- cor(survey_data_regression[cluster_indices, genre_cols], survey_data_regression[cluster_indices, "depression"])
}

# Add cluster assignments to your filtered data frame
survey_data_regression$genre_cluster <- as.factor(k2$cluster)

# Print the correlation matrix
print(correlation_matrix)


```

```{r}
library(pheatmap)
# Calculate means of each attribute for each cluster
cluster_means <- aggregate(survey_data_regression[,genre_cols], 
                           by=list(cluster=survey_data_regression$genre_cluster), 
                           FUN=mean)

# View the means
print(cluster_means)

# Generate the heatmap
pheatmap(cluster_means[-1],  # Exclude the cluster column from the heatmap
         scale = "row",  # Scale values by row
         clustering_distance_rows = "euclidean",
         clustering_distance_cols = "euclidean",
         clustering_method = "complete",
         main = "Heatmap of attribute means by genre_frequncy clusters",
         color = colorRampPalette(c("navy", "white", "firebrick3"))(50))

# group 1 - high dancabillty , high energy, hight loudness, moderate speechiness,low acousticness, very low instrumentlness 
# group 2 -  moderate energy, moderate loudness, low energy,high acousticness,low instumentalness
# group 3 - moderate dancabillty, moderate energy, low loudness,
```

```{r}

# Melt the data into a format suitable for ggplot
melted_df <- reshape2::melt(survey_data_regression, id.vars = "genre_cluster", measure.vars = c("depression", "ocd", "insomnia", "anxiety"))

# Create a boxplot for each pair of (cluster, mental problem)
ggplot(melted_df, aes(x=genre_cluster, y=value)) + 
  geom_boxplot(aes(fill=genre_cluster), outlier.shape = NA) +  # Hide outliers for better visualization
  facet_wrap(~variable, scales = "free", nrow = 4) +  # Use facets to arrange the plots
  theme_bw() + 
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"),
        strip.text = element_text(size = 14),
        legend.position = "none") +  # Hide legend
  labs(x = "Cluster", y = "Score", title = "Comparison of mental health scores across genere_frequncy clusters") +
  coord_cartesian(ylim = c(0, 10))  # Limit y axis to the range of mental health scores
```

```{r}
# Run an ANOVA to test if there is a significant difference in depression scores between clusters
anova_test <- aov(depression ~ as.factor(genre_cluster), data = survey_data_regression)
summary(anova_test)

# Bin depression scores into categories
survey_data_regression$depression_cat <- cut(survey_data_regression$depression, breaks = 3, labels = c("Low", "Medium", "High"))

# Form a contingency table
cont_table <- table(survey_data_regression$genre_cluster, survey_data_regression$depression_cat)

# Perform Chi-square test
chi_test <- chisq.test(cont_table)

# Get p-value
chi_test$p.value

```

**We are checking correlation between the frequncy clusters and depression**

```{r}

# Get unique cluster assignments
unique_clusters <- unique(k2$cluster)

# Create an empty matrix to store the correlation coefficients
correlation_matrix <- matrix(NA, nrow = length(unique_clusters), ncol = length(genre_cols))

# Assign 'depression_clusterX' as the row names
rownames(correlation_matrix) <- paste('depression_cluster', seq_len(length(unique_clusters)), sep = '')

# Assign genre_cols as the column names
colnames(correlation_matrix) <- genre_cols

# Calculate the correlation coefficient for each cluster's features with depression
for (i in 1:length(unique_clusters)) {
  cluster_indices <- which(k2$cluster == unique_clusters[i])
  for (j in 1:length(genre_cols)) {
    correlation_matrix[i, j] <- cor(survey_data_regression[cluster_indices, genre_cols[j]], survey_data_regression[cluster_indices, "depression"])
  }
}

# Print the correlation matrix
print(correlation_matrix)

```

**We saw that dimension reduction isnt giving us good results, we will try feature selection**

**We will calculate the spearman corrolation between each score to each one of the mental health oridinally features**

```{r, warning=FALSE}
# Assuming your data frame is named survey_data_classification
X_variables <- c('danceability_score', 'energy_score', 'loudness_score', 
                 'speechiness_score', 'acousticness_score', 'instrumentalness_score',
                 'liveness_score', 'valence_score', 'tempo_score')

Y_variables <- c("ocd", "depression", "insomnia", "anxiety")

# Initialize an empty data frame to store the correlation results
correlation_results <- data.frame()

# Calculate Spearman's rank correlation
for (x in X_variables) {
  for (y in Y_variables) {
    correlation <- cor.test(survey_data_regression[[x]], survey_data_regression[[y]], method = "spearman")
    result <- data.frame(
      X_variable = x,
      Y_variable = y,
      correlation = correlation$estimate,
      p_value = correlation$p.value
    )
    correlation_results <- rbind(correlation_results, result)
  }
}

# Filter the results
filtered_results <- correlation_results %>%
  filter(p_value < 0.05)

# Print the filtered results
print(filtered_results)

```

We do feature selection based on the attributes scores with the highest correlation

```{r, warning=FALSE}
# Assuming survey_data_regression is your dataframe
feature_select_categorical <- survey_data_regression
feature_select_categorical <- na.omit(feature_select_categorical)

# Make sure depression is an ordinal variable
feature_select_categorical$depression <- factor(feature_select_categorical$depression, ordered = TRUE)

# Categorical variable names
categorical_vars <- c("primary_streaming_service", "while_working", "instrumentalist", "composer", "fav_genre", "exploratory", "foreign_languages")

# Create a dataframe to store chi-square values
chi_square_results <- data.frame(variable = character(), chi_square = numeric(), p_value = numeric())

# Applying Chi-Square test
for(cat_var in categorical_vars){
  tbl <- table(as.matrix(feature_select_categorical[,cat_var]), feature_select_categorical$depression)
  test_result <- chisq.test(tbl)
  
  # Check the p-value and Chi-square
  print(paste("Chi-square test for", cat_var, ":"))
  print(paste("Chi-square =", test_result$statistic))
  print(paste("p-value =", test_result$p.value))

  # Store the results
  chi_square_results <- rbind(chi_square_results, data.frame(variable = cat_var, chi_square = as.numeric(test_result$statistic), p_value = test_result$p.value))
}

# Sort by chi-square value in descending order
chi_square_results <- chi_square_results[order(-chi_square_results$chi_square),]

# Print the sorted results
print(chi_square_results)

```

**fit to multiple linear regression model - using cross validation**

```{r}
set.seed(1003)
data_split = initial_split(survey_data_regression, prop = 0.8)
train_data_regression = training(data_split)
test_data_regression = testing(data_split)

cv_folds <- vfold_cv(train_data_regression, v = 10, strata = "depression")

# Define a recipe for preprocessing
rec <- recipe(depression ~ ., data = train_data_regression) %>%
  step_impute_mode(instrumentalist, while_working, music_effects, foreign_languages, primary_streaming_service ) %>%
  step_impute_median(age, bpm) %>%
  step_rm(timestamp, permissions, fav_genre, instrumentalist, composer, ocd, anxiety, insomnia,depression_cat,danceability_score,  speechiness_score,  instrumentalness_score, liveness_score, valence_score, tempo_score) %>%
  step_mutate(music_effects = as.factor(ifelse(music_effects %in% c("Worsen", "Improve"), 1, 0))) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors())

# Specify the model you want to use
lm_model <- 
  linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Combine the model and recipe into a workflow
lm_workflow <- 
  workflow() %>%
  add_recipe(rec) %>%
  add_model(lm_model)

set.seed(1001)
keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

set.seed(1003)
lm_res <- lm_workflow %>% fit_resamples(resamples = cv_folds, control = keep_pred)


# Collect predictions
predictions <- lm_res %>% collect_predictions()
predictions


# Calculate R-squared
rsq_result <- rsq(predictions, truth = "depression", estimate = ".pred")
print(rsq_result)

```
